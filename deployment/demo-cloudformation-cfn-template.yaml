AWSTemplateFormatVersion: '2010-09-09'
Description: >
  Author: Alick Li
  Purpose: Class Demonstration
  Info: This yaml template is used for DynamoDB, S3, and Lambda setup for data import (without S3 eventnotification)
Parameters:
  BucketName:
    Type: String
    Description: Name of the S3 bucket
    Default: demo-s3-dictationdatabase

  TableName:
    Type: String
    Description: Name of the DynamoDB table
    Default: demo-s3-dictationdatabase

  LambdaFunctionName:
    Type: String
    Description: Name of the Lambda function
    Default: demo-s3-dictationdatabase

Resources:
  MyDynamoDBTable:
    Type: AWS::DynamoDB::Table
    Properties:
      TableName: !Ref TableName
      AttributeDefinitions:
        - AttributeName: id
          AttributeType: S
      KeySchema:
        - AttributeName: id
          KeyType: HASH
      BillingMode: PAY_PER_REQUEST

  LambdaExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      Policies:
        - PolicyName: LambdaS3DynamoDBAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - s3:GetObject
                Resource:
                  - !Sub arn:aws:s3:::${BucketName}
                  - !Sub arn:aws:s3:::${BucketName}/*
              - Effect: Allow
                Action:
                  - dynamodb:PutItem
                  - dynamodb:DescribeTable
                Resource: !GetAtt MyDynamoDBTable.Arn
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
  MyLambdaFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Ref LambdaFunctionName
      Runtime: python3.11
      Role: !GetAtt LambdaExecutionRole.Arn
      Handler: index.handler
      Environment: 
        Variables:
          TABLE_NAME: !Ref TableName
          BUCKET_NAME: !Ref BucketName
      Code:
        ZipFile: |
          import json
          import boto3
          import os
          import re
          
          def handler(event, context):
              # Initialize a DynamoDB client
              table_name = os.environ['TABLE_NAME']
              bucket_name = os.environ['BUCKET_NAME']
              dynamodb = boto3.resource('dynamodb')
              table = dynamodb.Table(table_name)
          
              # Process each record in the event
              for record in event['Records']:
                  # Get the bucket name and key for the uploaded S3 object
                  s3_bucket_name = record['s3']['bucket']['name']
                  s3_object_key = record['s3']['object']['key']
          
                  # Check if the right file is being processed
                  if not s3_object_key.endswith('database.js'):
                      continue  # Skip processing if it's not the correct file
          
                  # Fetch data from S3
                  s3 = boto3.client('s3')
                  try:
                      response = s3.get_object(Bucket=s3_bucket_name, Key=s3_object_key)
                      raw_data = response['Body'].read().decode('utf-8')
          
                      # Use regular expression to extract JSON array
                      match = re.search(r'\[\s*{.*}\s*\]', raw_data, re.DOTALL)
                      if not match:
                          raise ValueError("No valid JSON array found in the file")
          
                      json_data = match.group()
          
                      # Parse the JSON data
                      items = json.loads(json_data)
          
                      # Add items to DynamoDB
                      for item in items:
                          table.put_item(Item=item)
                  except Exception as e:
                      print(f"Error processing file {s3_object_key} from bucket {s3_bucket_name}: {e}")
                      raise e
          
              return {
                  'statusCode': 200,
                  'body': json.dumps('Data import completed successfully')
              }
      Timeout: 120
      
Outputs:
  LambdaFunctionArn:
    Description: The ARN of the Lambda function
    Value: !GetAtt MyLambdaFunction.Arn

  DynamoDBTableArn:
    Description: The ARN of the DynamoDB table
    Value: !GetAtt MyDynamoDBTable.Arn

  S3BucketName:
    Description: The name of the S3 bucket
    Value: !Ref BucketName
